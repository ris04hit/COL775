{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T13:14:24.697408Z","iopub.status.busy":"2024-03-12T13:14:24.696883Z","iopub.status.idle":"2024-03-12T13:14:27.983413Z","shell.execute_reply":"2024-03-12T13:14:27.982487Z","shell.execute_reply.started":"2024-03-12T13:14:24.697360Z"},"trusted":true},"outputs":[],"source":["# imports\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import torch\n","import torchvision\n","import torchmetrics\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utils\n","\n","# Creating directory\n","def create_dir(addr):\n","    if not os.path.exists(addr):\n","        os.mkdir(addr)\n","\n","# Delete folder and its content\n","def remove_folder_contents(folder):\n","    for the_file in os.listdir(folder):\n","        file_path = os.path.join(folder, the_file)\n","        try:\n","            if os.path.isfile(file_path):\n","                os.unlink(file_path)\n","            elif os.path.isdir(file_path):\n","                remove_folder_contents(file_path)\n","                os.rmdir(file_path)\n","        except Exception as e:\n","            print(e)\n","\n","# Addresses\n","\n","# Raw data\n","raw_address = \"../input/indian-birds/Birds_25\"\n","raw_train = \"../input/indian-birds/Birds_25/train\"\n","raw_test = \"../input/indian-birds/Birds_25/test\"\n","raw_val = \"../input/indian-birds/Birds_25/val\"\n","\n","# Temp Address\n","temp_address = \"temp\"\n","\n","# Results\n","result_address = \"results/\"\n","base_model_result = \"results/base_model\"\n","bn_model_result = \"results/bn_model\"\n","in_model_result = \"results/in_model\"\n","bin_model_result = \"results/bin_model\"\n","ln_model_result = \"results/ln_model\"\n","gn_model_result = \"results/gn_model\"\n","nn_model_result = \"results/nn_model\"\n","bn_model_bsize8_result = \"results/bn_model_bsize8\"\n","gn_model_bsize8_result = \"results/gn_model_bsize8\"\n","bn_model_bsize128_result = \"results/bn_model_bsize128\"\n","gn_model_bsize128_result = \"results/gn_model_bsize128\"\n","overall_result_address = \"results/overall\"\n","bn_vs_base_result = \"results/bn_base\"\n","bn_vs_gn_bsize_result = \"results/bn_gn_bsize\"\n","\n","# Init Structure\n","create_dir(temp_address)\n","create_dir(raw_address)\n","create_dir(raw_train)\n","create_dir(raw_test)\n","create_dir(raw_val)\n","create_dir(result_address)\n","create_dir(overall_result_address)\n","create_dir(bn_vs_base_result)\n","create_dir(bn_vs_gn_bsize_result)\n","# remove_folder_contents(result_address)\n","\n","# Constants\n","random_seed = 68\n","n = 2\n","r = 25\n","batch_size = 32\n","\n","# Setting Random Seed\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cuda\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Working with {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data Loader\n","\n","# Finding Category\n","category = sorted(os.listdir(raw_train))\n","\n","# Transformation for preprocessing\n","transform_augment = torchvision.transforms.Compose([\n","                    torchvision.transforms.RandomHorizontalFlip(),\n","                    torchvision.transforms.RandomVerticalFlip(),\n","                    torchvision.transforms.RandomAutocontrast(),\n","                    torchvision.transforms.ToTensor()\n","                    ])\n","\n","transform_normal = torchvision.transforms.Compose([\n","                   torchvision.transforms.ToTensor()\n","                   ])\n","\n","class DataSet(torch.utils.data.Dataset):\n","    def __init__(self, address, augment = True):\n","        self.address = address\n","        self.cat_list = []\n","        for cat in category:\n","            cat_path = os.path.join(self.address, cat)\n","            self.cat_list.append(sorted(os.listdir(cat_path)))\n","        self.transform = transform_augment if augment else transform_normal\n","\n","    def __len__(self):\n","        return sum([len(elem) for elem in self.cat_list])\n","    \n","    def __getitem__(self, idx):\n","        ind = idx\n","        for cat_ind in range(len(self.cat_list)):\n","            if ind < len(self.cat_list[cat_ind]):\n","                cat_path = os.path.join(self.address, category[cat_ind])\n","                img_path = os.path.join(cat_path, self.cat_list[cat_ind][ind])\n","                break\n","            else:\n","                ind -= len(self.cat_list[cat_ind])\n","        img = torchvision.datasets.folder.default_loader(img_path)\n","        x = self.transform(img).to(torch.float)\n","        return x, cat_ind\n","\n","# Training Dataset and Data Loader\n","dataset_train = DataSet(raw_train, augment=False)\n","data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = 4)\n","\n","# Validation Dataset and Data Loader\n","dataset_val = DataSet(raw_val, augment=False)\n","data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers = 4)\n","\n","# Test Dataset and Data Loader\n","dataset_test = DataSet(raw_test, augment=False)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model\n","\n","class ResBlock(torch.nn.Module):\n","    def __init__(self, in_channel, out_channel, norm_func, kernel_size=3, stride=1):\n","        super(ResBlock, self).__init__()\n","        \n","        self.conv1 = torch.nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding=1, dtype=torch.float)\n","        self.norm1 = norm_func(out_channel, dtype=torch.float)\n","        self.activation1 = torch.nn.ReLU()\n","        \n","        self.conv2 = torch.nn.Conv2d(out_channel, out_channel, kernel_size, stride=1, padding=1, dtype=torch.float)\n","        self.norm2 = norm_func(out_channel, dtype=torch.float)\n","        self.activation2 = torch.nn.ReLU()\n","        \n","        self.project = True if (in_channel != out_channel) or (stride != 1) else False\n","        if self.project:\n","            self.conv_project = torch.nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding=1, dtype=torch.float)\n","\n","    def forward(self, x):\n","        res = x\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x = self.activation1(x)\n","        \n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x += self.conv_project(res) if self.project else res\n","        x = self.activation2(x)\n","        \n","        return x\n","\n","class Resnet(torch.nn.Module):\n","    def __init__(self, n, r, norm_func = torch.nn.BatchNorm2d):\n","        super(Resnet, self).__init__()\n","\n","        self.norm = norm_func\n","        \n","        #Input\n","        self.input_layer = []\n","        self.input_layer.append(torch.nn.Conv2d(3, 16, 3, 1, padding=1, dtype=torch.float))\n","        self.input_layer.append(self.norm(16, dtype=torch.float))\n","        self.input_layer.append(torch.nn.ReLU())\n","        self.input_layer = torch.nn.Sequential(*self.input_layer)\n","        \n","        # Layer1\n","        self.hidden_layer1 = []\n","        for i in range(n):\n","            self.hidden_layer1.append(ResBlock(16, 16, self.norm))\n","        self.hidden_layer1 = torch.nn.Sequential(*self.hidden_layer1)\n","        \n","        # Layer2\n","        self.hidden_layer2 = []\n","        self.hidden_layer2.append(ResBlock(16, 32, self.norm, stride = 2))\n","        for i in range(n-1):\n","            self.hidden_layer2.append(ResBlock(32, 32, self.norm))\n","        self.hidden_layer2 = torch.nn.Sequential(*self.hidden_layer2)\n","        \n","        # Layer3\n","        self.hidden_layer3 = []\n","        self.hidden_layer3.append(ResBlock(32, 64, self.norm, stride = 2))\n","        for i in range(n-1):\n","            self.hidden_layer3.append(ResBlock(64, 64, self.norm))\n","        self.hidden_layer3 = torch.nn.Sequential(*self.hidden_layer3)\n","            \n","        # Pool Layer\n","        self.pool = torch.nn.AdaptiveAvgPool2d(1)\n","        self.flatten = torch.nn.Flatten()\n","        \n","        # Output Layer\n","        self.output_layer = torch.nn.Linear(64, r, dtype=torch.float)\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        x = self.hidden_layer1(x)\n","        x = self.hidden_layer2(x)\n","        x = self.hidden_layer3(x)\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","        x = self.output_layer(x)\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T13:14:28.038373Z","iopub.status.busy":"2024-03-12T13:14:28.038037Z","iopub.status.idle":"2024-03-12T13:14:28.072909Z","shell.execute_reply":"2024-03-12T13:14:28.071853Z","shell.execute_reply.started":"2024-03-12T13:14:28.038344Z"},"trusted":true},"outputs":[],"source":["# Training Model\n","\n","def train(model, data_loader, save_addr, num_epoch = 50, learning_rate = 1e-3, overwrite = False):\n","    # Creating save folder\n","    create_dir(save_addr)\n","    model_addr = os.path.join(save_addr, 'model')\n","    create_dir(model_addr)\n","    loss_addr = os.path.join(save_addr, 'loss')\n","    create_dir(loss_addr)\n","\n","    # Setting model to train mode\n","    model.train()\n","\n","    # Parameters for training\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","    start_time = time.time()\n","\n","    # Metrics\n","    metric_f1_micro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'micro').to(device)\n","    metric_f1_macro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'macro').to(device)\n","    metric_accuracy = torchmetrics.classification.Accuracy(task = 'multiclass', num_classes = r).to(device)\n","\n","    for epoch in range(num_epoch):\n","        batch_ct = 0\n","        epoch_loss = 0\n","        label_arr = torch.tensor([], device=device)\n","        label_pred_arr = torch.tensor([], device=device)\n","\n","        # Loading previous model\n","        epoch_addr = os.path.join(model_addr, f'{0 if epoch < 10 else \"\"}{epoch}.pt')\n","        epoch_loss_addr = os.path.join(loss_addr, f'{0 if epoch < 10 else \"\"}{epoch}.pt')\n","\n","        if not overwrite and os.path.exists(epoch_addr) and os.path.exists(epoch_loss_addr):\n","            model.load_state_dict(torch.load(epoch_addr))\n","            model.train()\n","            loss_arr = torch.load(epoch_loss_addr)\n","            epoch_loss = loss_arr[0].item()\n","            accuracy = loss_arr[1].item()\n","            f1_micro = loss_arr[2].item()\n","            f1_macro = loss_arr[3].item()\n","\n","            print(f\"Epoch: {epoch} Loaded\\t\\tLoss: {epoch_loss}\\tAccuracy: {accuracy}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\")\n","        else:\n","            # Training next epoch\n","            for x, y in data_loader:\n","                # To Device\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                # Predictions\n","                y_pred = model(x)\n","\n","                # Maintaining label array\n","                label, label_pred = y, torch.argmax(y_pred, dim=1)\n","                label_arr = torch.cat((label_arr, label))\n","                label_pred_arr = torch.cat((label_pred_arr, label_pred))\n","\n","                # Calculating Loss\n","                loss = loss_fn(y_pred, y)\n","                epoch_loss += loss.item()\n","\n","                # Back Propogation\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Log\n","                batch_ct += 1\n","\n","            # Computing Metrics\n","            f1_micro = metric_f1_micro(label_pred_arr, label_arr).item()\n","            f1_macro = metric_f1_macro(label_pred_arr, label_arr).item()\n","            accuracy = metric_accuracy(label_pred_arr, label_arr).item()\n","            loss_arr = torch.tensor([epoch_loss/batch_ct, accuracy, f1_micro, f1_macro])\n","\n","            # Saving model after each epoch\n","            torch.save(model.state_dict(), epoch_addr)\n","            torch.save(loss_arr, epoch_loss_addr)\n","\n","            print(f\"Epoch: {epoch}\\tLoss: {epoch_loss/batch_ct}\\tAccuracy: {accuracy}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\\tTime: {time.time() - start_time}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Validate Model\n","\n","def validate(model, data_loader, save_addr, load_addr, overwrite = False):\n","    # Creating save folder\n","    create_dir(save_addr)\n","    model_addr_load = os.path.join(load_addr, 'model')\n","\n","    # Parameters for training\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    start_time = time.time()\n","\n","    # Metrics\n","    metric_f1_micro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'micro').to(device)\n","    metric_f1_macro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'macro').to(device)\n","    metric_accuracy = torchmetrics.classification.Accuracy(task = 'multiclass', num_classes = r).to(device)\n","\n","    epoch = 0\n","    for param_addr in sorted(os.listdir(model_addr_load)):\n","        # Save Address\n","        epoch_loss_addr = os.path.join(save_addr, param_addr)\n","\n","        # Checking if already present\n","        if not overwrite and os.path.exists(epoch_loss_addr):\n","            loss_arr = torch.load(epoch_loss_addr)\n","            epoch_loss = loss_arr[0].item()\n","            accuracy = loss_arr[1].item()\n","            f1_micro = loss_arr[2].item()\n","            f1_macro = loss_arr[3].item()\n","\n","            print(f\"Epoch: {epoch} Loaded\\t\\tLoss: {epoch_loss}\\tAccuracy: {accuracy}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\")\n","            epoch += 1\n","            continue\n","        \n","        # Initializing Variable\n","        batch_ct = 0\n","        epoch_loss = 0\n","        label_arr = torch.tensor([], device=device)\n","        label_pred_arr = torch.tensor([], device=device)\n","\n","        # Loading Model\n","        model.load_state_dict(torch.load(os.path.join(model_addr_load, param_addr)))\n","\n","        # Evaluate Model and freezing it\n","        model.eval()\n","        with torch.no_grad():\n","            for x, y in data_loader:\n","                # To Device\n","                x = x.to(device)\n","                y = y.to(device)\n","                \n","                # Predictions\n","                y_pred = model(x)\n","\n","                # Maintaining label array\n","                label, label_pred = y, torch.argmax(y_pred, dim=1)\n","                label_arr = torch.cat((label_arr, label))\n","                label_pred_arr = torch.cat((label_pred_arr, label_pred))\n","\n","                # Calculating Loss\n","                loss = loss_fn(y_pred, y)\n","                epoch_loss += loss.item()\n","\n","                # Log\n","                batch_ct += 1\n","\n","        # Computing Metrics\n","        f1_micro = metric_f1_micro(label_pred_arr, label_arr).item()\n","        f1_macro = metric_f1_macro(label_pred_arr, label_arr).item()\n","        accuracy = metric_accuracy(label_pred_arr, label_arr).item()\n","        loss_arr = torch.tensor([epoch_loss/batch_ct, accuracy, f1_micro, f1_macro])\n","\n","        # Saving model after each epoch\n","        torch.save(loss_arr, epoch_loss_addr)\n","\n","        # Log\n","        print(f\"Epoch: {epoch}\\tLoss: {epoch_loss/batch_ct}\\tAccuracy: {accuracy}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\\tTime: {time.time() - start_time}\")\n","        epoch += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot\n","def save(arr_x, arr_train, arr_val, address, title, y_label, y_lim = None):\n","    fig, ax = plt.subplots()\n","    ax.plot(arr_x, arr_train, label = 'Train')\n","    ax.plot(arr_x, arr_val, label = 'Validation')\n","    ax.set_xlabel(\"num_epochs\")\n","    ax.set_ylabel(y_label)\n","    ax.set_title(title)\n","    if y_lim:\n","        ax.set_ylim(y_lim[0], y_lim[1])\n","    ax.legend()\n","    plt.savefig(address)\n","\n","def plot(train_address, val_address, save_address, name):\n","    # Loading metrics\n","    train_metrics = []\n","    val_metrics = []\n","    for epoch_name in sorted(os.listdir(train_address)):\n","        train_metrics.append(np.array(torch.load(os.path.join(train_address, epoch_name))))\n","        val_metrics.append(np.array(torch.load(os.path.join(val_address, epoch_name))))\n","    train_metrics = np.stack(train_metrics)\n","    val_metrics = np.stack(val_metrics)\n","\n","    arr_x = np.arange(1, train_metrics.shape[0]+1)\n","    save(arr_x, train_metrics[:, 0], val_metrics[:, 0], os.path.join(save_address, 'loss'), f\"{name}: Cross Entropy Loss\", \"Loss\")\n","    save(arr_x, 100*train_metrics[:, 1], 100*val_metrics[:, 1], os.path.join(save_address, 'acc'), f\"{name}: Accuracy\", \"Accuracy\")\n","    save(arr_x, train_metrics[:, 2], val_metrics[:, 2], os.path.join(save_address, 'micro'), f\"{name}: F1 Micro Score\", \"f1_micro\")\n","    save(arr_x, train_metrics[:, 3], val_metrics[:, 3], os.path.join(save_address, 'macro'), f\"{name}: F1 Macro Score\", \"f1_macro\")\n","\n","    return np.argmin(val_metrics[:, 0]), train_metrics, val_metrics\n","\n","# Report\n","def report(address, ind):\n","    metric_arr = []\n","    for folder_name in ['loss', 'val', 'test']:\n","        folder_address = os.path.join(address, folder_name)\n","        metric_addr = os.path.join(folder_address, sorted(os.listdir(folder_address))[ind])\n","        metric_arr.append(np.array(torch.load(metric_addr)))\n","\n","    with open(os.path.join(address, 'result.txt'), 'w') as file:\n","        file.write(\"Training:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[0][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[0][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[0][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[0][3]}\\n\")\n","        file.write(\"\\nValidation:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[1][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[1][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[1][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[1][3]}\\n\")\n","        file.write(\"\\nTesting:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[2][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[2][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[2][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[2][3]}\\n\")\n","    \n","    return metric_arr\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model Function\n","\n","def model_func(model, address, name):\n","    train(model, data_loader_train, address)\n","    validate(model, data_loader_val, os.path.join(address, 'val'), address)\n","    validate(model, data_loader_test, os.path.join(address, 'test'), address)\n","    best_model_ind, train_metric, val_metric = plot(os.path.join(address, 'loss'), os.path.join(address, 'val'), address, name)\n","    best_model_metric = report(address, best_model_ind)\n","    return train_metric, val_metric, best_model_metric\n","\n","# Base Model\n","\n","resnet_base_model = Resnet(n, r).to(device)\n","base_model_metric_train, base_model_metric_val, base_model_metric_best = model_func(resnet_base_model, base_model_result, 'Base Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Batch Normalization\n","\n","class BN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, dtype = torch.float, momentum = 0.9, epsilon = 1e-8):\n","        super(BN_norm, self).__init__()\n","\n","        # Class Constants\n","        self.dtype = dtype\n","        self.momentum = momentum\n","        self.epsilon = epsilon\n","\n","        # Model Parameters\n","        self.mean = torch.nn.Parameter(torch.zeros(channel_num, dtype=dtype))\n","        self.std = torch.nn.Parameter(torch.ones(channel_num, dtype=dtype))\n","\n","        # Running Parameters\n","        self.register_buffer('running_mean', torch.zeros((1, channel_num, 1, 1), dtype=dtype))\n","        self.register_buffer('running_var', torch.ones((1, channel_num, 1, 1), dtype=dtype))\n","    \n","    def forward(self, x):\n","        if self.training:\n","            # Calculating Batch Stats\n","            batch_mean = torch.mean(x, (0, 2, 3), keepdim=True)\n","            batch_var = torch.var(x, (0, 2, 3), unbiased=False, keepdim=True)\n","\n","            # Normalizing Input\n","            x = (x - batch_mean)/torch.sqrt(batch_var + self.epsilon)\n","\n","            # Updating running statistics\n","            self.running_mean = self.momentum*self.running_mean + (1-self.momentum)*batch_mean\n","            self.running_var = self.momentum*self.running_var + (1-self.momentum)*batch_var\n","        \n","        else:\n","            # Normalizing Input\n","            x = (x - self.running_mean)/torch.sqrt(self.running_var + self.epsilon)\n","        \n","        # Scale and shift\n","        x = x*self.std.view(1, -1, 1, 1) + self.mean.view(1, -1, 1, 1)\n","\n","        return x\n","    \n","resnet_bn_model = Resnet(n, r, norm_func=BN_norm).to(device)\n","bn_model_metric_train, bn_model_metric_val, bn_model_metric_best = model_func(resnet_bn_model, bn_model_result, 'BN Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instance Normalization\n","\n","class IN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, dtype = torch.float,  epsilon = 1e-8):\n","        super(IN_norm, self).__init__()\n","\n","        # Class Constants\n","        self.dtype = dtype\n","        self.epsilon = epsilon\n","\n","    def forward(self, x):\n","        # Calculating Batch Stats\n","        batch_mean = torch.mean(x, (2, 3), keepdim=True)\n","        batch_var = torch.var(x, (2, 3), unbiased=False, keepdim=True)\n","\n","        # Normalizing Input\n","        x = (x - batch_mean)/torch.sqrt(batch_var + self.epsilon)\n","\n","        return x\n","    \n","resnet_in_model = Resnet(n, r, norm_func=IN_norm).to(device)\n","in_model_metric_train, in_model_metric_val, in_model_metric_best = model_func(resnet_in_model, in_model_result, 'IN Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Batch Instance Normalization\n","\n","class BIN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, dtype = torch.float, momentum = 0.9, epsilon = 1e-8):\n","        super(BIN_norm, self).__init__()\n","\n","        # Class Constants\n","        self.dtype = dtype\n","        self.momentum = momentum\n","        self.epsilon = epsilon\n","\n","        # Model Parameters\n","        self.mean = torch.nn.Parameter(torch.zeros((1, channel_num, 1, 1), dtype=dtype))\n","        self.std = torch.nn.Parameter(torch.ones((1, channel_num, 1, 1), dtype=dtype))\n","        self.rho = torch.nn.Parameter(torch.rand((1, channel_num, 1, 1), dtype=dtype))\n","\n","        # Running Parameters\n","        self.register_buffer('running_mean', torch.zeros((1, channel_num, 1, 1), dtype=dtype))\n","        self.register_buffer('running_var', torch.ones((1, channel_num, 1, 1), dtype=dtype))\n","    \n","    def forward(self, x):\n","        # Clipping rho\n","        with torch.no_grad():\n","            self.rho = torch.nn.Parameter(torch.clamp(self.rho, min=0, max=1))\n","\n","        # Batch Normalization\n","        if self.training:\n","            # Calculating Batch Stats\n","            batch_mean = torch.mean(x, (0, 2, 3), keepdim=True)\n","            batch_var = torch.var(x, (0, 2, 3), unbiased=False, keepdim=True)\n","\n","            # Normalizing Input\n","            x_bn = (x - batch_mean)/torch.sqrt(batch_var + self.epsilon)\n","\n","            # Updating running statistics\n","            self.running_mean = self.momentum*self.running_mean + (1-self.momentum)*batch_mean\n","            self.running_var = self.momentum*self.running_var + (1-self.momentum)*batch_var\n","        \n","        else:\n","            # Normalizing Input\n","            x_bn = (x - self.running_mean)/torch.sqrt(self.running_var + self.epsilon)\n","        \n","        # Instance Normalization\n","        # Calculating Batch Stats\n","        batch_mean = torch.mean(x, (2, 3), keepdim=True)\n","        batch_var = torch.var(x, (2, 3), unbiased=False, keepdim=True)\n","\n","        # Normalizing Input\n","        x_in = (x - batch_mean)/torch.sqrt(batch_var + self.epsilon)\n","\n","        # Combining BN and IN\n","        x = self.rho*x_bn + (1-self.rho)*x_in\n","\n","        # Scale and shift\n","        x = x*self.std + self.mean\n","\n","        return x\n","    \n","resnet_bin_model = Resnet(n, r, norm_func=BIN_norm).to(device)\n","bin_model_metric_train, bin_model_metric_val, bin_model_metric_best = model_func(resnet_bin_model, bin_model_result, 'BIN Model')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Layer Normalization\n","\n","class LN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, dtype = torch.float, epsilon = 1e-8):\n","        super(LN_norm, self).__init__()\n","\n","        # Class Constants\n","        self.dtype = dtype\n","        self.epsilon = epsilon\n","\n","        # Model Parameters\n","        self.mean = torch.nn.Parameter(torch.zeros((1, channel_num, 1, 1), dtype=self.dtype))\n","        self.std = torch.nn.Parameter(torch.ones((1, channel_num, 1, 1), dtype=self.dtype))\n","\n","    def forward(self, x):\n","        # Calculating Batch Stats\n","        batch_mean = torch.mean(x, (1, 2, 3), keepdim=True)\n","        batch_var = torch.var(x, (1, 2, 3), unbiased=False, keepdim=True)\n","\n","        # Normalizing Input\n","        x = (x - batch_mean)/torch.sqrt(batch_var + self.epsilon)\n","\n","        # Scale and shift\n","        x = x*self.std + self.mean\n","\n","        return x\n","\n","\n","resnet_ln_model = Resnet(n, r, norm_func=LN_norm).to(device)\n","ln_model_metric_train, ln_model_metric_val, ln_model_metric_best = model_func(resnet_ln_model, ln_model_result, 'LN Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Group Normalization\n","\n","class GN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, group_num = 8, dtype = torch.float, epsilon = 1e-8):\n","        super(GN_norm, self).__init__()\n","\n","        # Class Constants\n","        self.dtype = dtype\n","        self.epsilon = epsilon\n","        self.g = group_num\n","\n","        # Model Parameters\n","        self.mean = torch.nn.Parameter(torch.zeros((1, channel_num, 1, 1), dtype=self.dtype))\n","        self.std = torch.nn.Parameter(torch.ones((1, channel_num, 1, 1), dtype=self.dtype))\n","\n","    def forward(self, x):\n","        # Reshaping\n","        n, c, h, w = x.shape\n","        x = torch.reshape(x, (n, self.g, c//self.g, h, w))\n","\n","        # Calculating Batch Stats\n","        grp_mean = torch.mean(x, (2, 3, 4), keepdim=True)\n","        grp_var = torch.var(x, (2, 3, 4), unbiased=False, keepdim=True)\n","\n","        # Normalizing Input\n","        x = (x - grp_mean)/torch.sqrt(grp_var + self.epsilon)\n","\n","        # Reshaping to original shape\n","        x = torch.reshape(x, (n, c, h, w))\n","\n","        # Scale and shift\n","        x = x*self.std + self.mean\n","\n","        return x\n","\n","resnet_gn_model = Resnet(n, r, norm_func=GN_norm).to(device)\n","gn_model_metric_train, gn_model_metric_val, gn_model_metric_best = model_func(resnet_gn_model, gn_model_result, 'GN Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# No Normalization\n","\n","class NN_norm(torch.nn.Module):\n","    def __init__(self, channel_num, dtype = torch.float):\n","        super(NN_norm, self).__init__()\n","        self.dtype = dtype\n","    \n","    def forward(self, x):\n","        return x.to(self.dtype)\n","\n","resnet_nn_model = Resnet(n, r, norm_func=NN_norm).to(device)\n","nn_model_metric_train, nn_model_metric_val, nn_model_metric_best = model_func(resnet_nn_model, nn_model_result, 'NN Model')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Variation with Batch Size\n","\n","# Batch Size: 8\n","data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers = 4)\n","data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=8, shuffle=False, num_workers = 4)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers = 4)\n","\n","resnet_bn_model_bsize8 = Resnet(n, r, norm_func=BN_norm).to(device)\n","bn_model_bsize8_metric_train, bn_model_bsize8_metric_val, bn_model_bsize8_metric_best = model_func(resnet_bn_model_bsize8, bn_model_bsize8_result, 'BN Model, Batch Size = 8')\n","\n","resnet_gn_model_bsize8 = Resnet(n, r, norm_func=GN_norm).to(device)\n","gn_model_bsize8_metric_train, gn_model_bsize8_metric_val, gn_model_bsize8_metric_best = model_func(resnet_gn_model_bsize8, gn_model_bsize8_result, 'GN Model, Batch Size = 8')\n","\n","# Batch Size: 128\n","data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True, num_workers = 4)\n","data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128, shuffle=False, num_workers = 4)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=False, num_workers = 4)\n","\n","resnet_bn_model_bsize128 = Resnet(n, r, norm_func=BN_norm).to(device)\n","bn_model_bsize128_metric_train, bn_model_bsize128_metric_val, bn_model_bsize128_metric_best = model_func(resnet_bn_model_bsize128, bn_model_bsize128_result, 'BN Model, Batch Size = 128')\n","\n","resnet_gn_model_bsize128 = Resnet(n, r, norm_func=GN_norm).to(device)\n","gn_model_bsize128_metric_train, gn_model_bsize128_metric_val, gn_model_bsize128_metric_best = model_func(resnet_gn_model_bsize128, gn_model_bsize128_result, 'GN Model, Batch Size = 128')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save Overall Plots\n","def save_overall(address, arr_x, metric, legend, title, y_label, y_lim = None):\n","    fig, ax = plt.subplots()\n","    for ind in range(metric.shape[0]):\n","        ax.plot(arr_x, metric[ind], label = legend[ind])\n","    ax.set_xlabel(\"num_epochs\")\n","    ax.set_ylabel(y_label)\n","    ax.set_title(title)\n","    if y_lim:\n","        ax.set_ylim(y_lim[0], y_lim[1])\n","    ax.legend()\n","    plt.savefig(address)\n","    return\n","\n","def save_comp(overall_result_address, arr_x, metric_train, metric_val, legend):\n","    save_overall(os.path.join(overall_result_address, 'train_acc'), arr_x, 100*metric_train[:, :, 1], legend, 'Train Accuracy', \"Accuracy\")\n","    save_overall(os.path.join(overall_result_address, 'val_acc'), arr_x, 100*metric_val[:, :, 1], legend, 'Validation Accuracy', \"Accuracy\")\n","    save_overall(os.path.join(overall_result_address, 'train_loss'), arr_x, metric_train[:, :, 0], legend, 'Train Loss', \"Loss\")\n","    save_overall(os.path.join(overall_result_address, 'val_loss'), arr_x, metric_val[:, :, 0], legend, 'Validation Loss', \"Loss\")\n","    save_overall(os.path.join(overall_result_address, 'train_f1_micro'), arr_x, metric_train[:, :, 2], legend, 'Train F1 Micro', \"F1 Micro\")\n","    save_overall(os.path.join(overall_result_address, 'val_f1_micro'), arr_x, metric_val[:, :, 2], legend, 'Validation F1 Micro', \"F1 Micro\")\n","    save_overall(os.path.join(overall_result_address, 'train_f1_macro'), arr_x, metric_train[:, :, 3], legend, 'Train F1 Macro', \"F1 Macro\")\n","    save_overall(os.path.join(overall_result_address, 'val_f1_macro'), arr_x, metric_val[:, :, 3], legend, 'Validation F1 Macro', \"F1 Macro\")\n","\n","# Overall Metric\n","metric_train = np.stack([bn_model_metric_train, in_model_metric_train, bin_model_metric_train, ln_model_metric_train, gn_model_metric_train, nn_model_metric_train])\n","metric_val = np.stack([bn_model_metric_val, in_model_metric_val, bin_model_metric_val, ln_model_metric_val, gn_model_metric_val, nn_model_metric_val])\n","metric_best = np.stack([bn_model_metric_best, in_model_metric_best, bin_model_metric_best, ln_model_metric_best, gn_model_metric_best, nn_model_metric_best])\n","legend = ['BN Model', 'IN Model', 'BIN Model', 'LN Model', 'GN Model', 'NN Model']\n","arr_x = np.arange(1, metric_train.shape[1]+1)\n","save_comp(overall_result_address, arr_x, metric_train, metric_val, legend)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compare Base Model with BN model\n","\n","metric_train = np.stack([base_model_metric_train, bn_model_metric_train])\n","metric_val = np.stack([base_model_metric_val, bn_model_metric_val])\n","metric_best = np.stack([base_model_metric_best, bn_model_metric_best])\n","legend = ['Pytorch Batchnorm', 'Batchnorm From Scratch']\n","arr_x = np.arange(1, metric_train.shape[1]+1)\n","save_comp(bn_vs_base_result, arr_x, metric_train, metric_val, legend)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Effect of Batch Size\n","\n","metric_train = np.stack([bn_model_bsize8_metric_train, bn_model_bsize128_metric_train, gn_model_bsize8_metric_train, gn_model_bsize128_metric_train])\n","metric_val = np.stack([bn_model_bsize8_metric_val, bn_model_bsize128_metric_val, gn_model_bsize8_metric_val, gn_model_bsize128_metric_val])\n","metric_best = np.stack([bn_model_bsize8_metric_best, bn_model_bsize128_metric_best, gn_model_bsize8_metric_best, gn_model_bsize128_metric_best])\n","legend = ['BN, Batch Size = 8', 'BN, Batch Size = 128', 'GN, Batch Size = 8', 'GN, Batch Size = 128']\n","arr_x = np.arange(1, metric_train.shape[1]+1)\n","save_comp(bn_vs_gn_bsize_result, arr_x, metric_train, metric_val, legend)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4584966,"sourceId":7824688,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":4}
