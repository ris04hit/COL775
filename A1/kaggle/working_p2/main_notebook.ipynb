{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtext\n",
    "import transformers\n",
    "import time\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath('../input/math-problem/data'))\n",
    "import evaluator\n",
    "\n",
    "os.environ[\"HF_HUB_DISAYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Creating directory\n",
    "def create_dir(addr):\n",
    "    if not os.path.exists(addr):\n",
    "        os.mkdir(addr)\n",
    "\n",
    "# Delete folder and its content\n",
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Addresses\n",
    "\n",
    "# Raw data\n",
    "raw_address = \"../input/math-problem/data\"\n",
    "raw_train = \"../input/math-problem/data/train.json\"\n",
    "raw_test = \"../input/math-problem/data/test.json\"\n",
    "raw_dev = \"../input/math-problem/data/dev.json\"\n",
    "\n",
    "# Model\n",
    "result = \"results/\"\n",
    "model_glove_add = \"results/model_glove\"\n",
    "model_glove_attention_add1 = \"results/model_glove_attention_0.3\"\n",
    "model_glove_attention_add2 = \"results/model_glove_attention_0.6\"\n",
    "model_glove_attention_add3 = \"results/model_glove_attention_0.9\"\n",
    "model_bert_frozen_add = \"results/model_bert_frozen\"\n",
    "model_bert_adaptive_add = \"results/model_bert_adaptive\"\n",
    "\n",
    "# Temp\n",
    "temp = \"temp/\"\n",
    "\n",
    "# Creating Directory\n",
    "create_dir(result)\n",
    "create_dir(temp)\n",
    "create_dir(model_glove_add)\n",
    "create_dir(model_glove_attention_add1)\n",
    "create_dir(model_glove_attention_add2)\n",
    "create_dir(model_glove_attention_add3)\n",
    "create_dir(model_bert_frozen_add)\n",
    "create_dir(model_bert_adaptive_add)\n",
    "\n",
    "# HyperParameters\n",
    "embedding_size = 200\n",
    "hidden_size = 256\n",
    "lr = 1e-3\n",
    "num_epoch = 40\n",
    "dropout = 0.5\n",
    "n_layers = 2\n",
    "clip = 1.0\n",
    "max_out_size = 100\n",
    "\n",
    "# Special Tokens\n",
    "NUM = \"<NUM>\"\n",
    "UNK = \"<UNK>\"\n",
    "SOS = \"<SOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "BERT_PAD_VALUE = 0\n",
    "\n",
    "# Random Seed\n",
    "torch.random.manual_seed(68)\n",
    "random.seed(68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Working with {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataSet, DataLoader and Vocab\n",
    "\n",
    "def remove_commas_from_numbers(text):\n",
    "    # Define a regular expression pattern to match numbers with commas\n",
    "    pattern = r'(\\d{1,3}(,\\d{3})*)'\n",
    "    \n",
    "    # Replace commas in numbers with empty string\n",
    "    result = re.sub(pattern, lambda x: x.group(0).replace(',', ''), text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_vocab(data, threshold = 3):\n",
    "    x_vocab, y_vocab = {}, set()\n",
    "    for dat in data:\n",
    "        for word in dat['Problem'].split():\n",
    "            word = word.lower()\n",
    "            if not word or all(char.isdigit() or char=='.' for char in word):\n",
    "                continue\n",
    "            if word in x_vocab:\n",
    "                x_vocab[word] += 1\n",
    "            else:\n",
    "                x_vocab[word] = 1\n",
    "        for word in dat['linear_formula'].split(\"|\"):\n",
    "            word = word.lower()\n",
    "            if not word:\n",
    "                continue\n",
    "            y_vocab.add(word)\n",
    "    x_vocab = sorted([word for word in x_vocab if x_vocab[word]>=threshold])\n",
    "    x_vocab.extend([SOS, EOS, PAD, UNK, NUM])\n",
    "    x_stoi = {x_vocab[ind]: ind for ind in range(len(x_vocab))}\n",
    "    y_vocab = sorted(list(y_vocab))\n",
    "    y_vocab.extend([SOS, EOS, PAD, UNK])\n",
    "    y_stoi = {y_vocab[ind]: ind for ind in range(len(y_vocab))}\n",
    "    return x_vocab, x_stoi, y_vocab, y_stoi\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, \"rb\") as file:\n",
    "            self.data = pd.DataFrame(json.load(file))\n",
    "        self.data['Problem'] = self.data['Problem'].apply(remove_commas_from_numbers)\n",
    "        self.data['Problem'] = self.data['Problem'].apply(lambda x: x.lower())\n",
    "        self.data = self.data.to_dict(orient='records')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret_data = self.data[idx]\n",
    "        return ret_data\n",
    "    \n",
    "    def tokenize(self, x_stoi, y_stoi):\n",
    "        bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        for dat in self.data:\n",
    "            bert_sentence = []\n",
    "            dat['x'] = [x_stoi[SOS]]\n",
    "            for word in dat['Problem'].split():\n",
    "                if word in x_stoi:\n",
    "                    dat['x'].append(x_stoi[word])\n",
    "                    bert_sentence.append(word)\n",
    "                elif all(char.isdigit() or char=='.' for char in word):\n",
    "                    dat['x'].append(x_stoi[NUM])\n",
    "                    bert_sentence.append(NUM)\n",
    "                else:\n",
    "                    dat['x'].append(x_stoi[UNK])\n",
    "                    bert_sentence.append(UNK)\n",
    "            bert_sentence = \" \".join(bert_sentence)\n",
    "            dat['bert_x'] = bert_tokenizer(bert_sentence)\n",
    "            dat['bert_x']['input_ids'] = torch.tensor(dat['bert_x']['input_ids'])\n",
    "            dat['bert_x']['attention_mask'] = torch.tensor(dat['bert_x']['attention_mask'])\n",
    "            dat['x'].append(x_stoi[EOS])\n",
    "            dat['x'] = torch.tensor(dat['x'])\n",
    "            if 'linear_formula' in dat:\n",
    "                dat['y'] = [y_stoi[word] if word in y_stoi else y_stoi[UNK] for word in dat['linear_formula'].split(\"|\")]\n",
    "            else:\n",
    "                dat['y'] = [y_stoi[UNK] for _ in range(max_out_size)]\n",
    "            dat['y'] = [y_stoi[SOS]] + dat['y'] + [y_stoi[EOS]]\n",
    "            dat['y'] = torch.tensor(dat['y'])\n",
    "            \n",
    "class DataLoader:\n",
    "    def __init__(self, batch_size = 256, shuffle = True):\n",
    "        self.data_train = Dataset(raw_train)\n",
    "        self.x_vocab, self.x_stoi, self.y_vocab, self.y_stoi = create_vocab(self.data_train.data)\n",
    "        self.data_train.tokenize(self.x_stoi, self.y_stoi)\n",
    "        self.train = torch.utils.data.DataLoader(self.data_train, batch_size = batch_size, collate_fn = self.collate, shuffle = shuffle)\n",
    "\n",
    "        self.data_dev = Dataset(raw_dev)\n",
    "        self.data_dev.tokenize(self.x_stoi, self.y_stoi)\n",
    "        self.dev = torch.utils.data.DataLoader(self.data_dev, batch_size = batch_size, collate_fn = self.collate, shuffle = False)\n",
    "\n",
    "        self.data_test = Dataset(raw_test)\n",
    "        self.data_test.tokenize(self.x_stoi, self.y_stoi)\n",
    "        self.test = torch.utils.data.DataLoader(self.data_test, batch_size = batch_size, collate_fn = self.collate, shuffle = False)\n",
    "\n",
    "    def collate(self, batch):\n",
    "        batch_x = torch.nn.utils.rnn.pad_sequence([example['x'] for example in batch], padding_value=self.x_stoi[PAD])\n",
    "        batch_y = torch.nn.utils.rnn.pad_sequence([example['y'] for example in batch], padding_value=self.y_stoi[PAD])\n",
    "        # Taking pad value of bert to be zero (default value)\n",
    "        batch_bert_x = {'x': torch.nn.utils.rnn.pad_sequence([example['bert_x']['input_ids'] for example in batch], padding_value=BERT_PAD_VALUE),\n",
    "                        'a_mask': torch.nn.utils.rnn.pad_sequence([example['bert_x']['attention_mask'] for example in batch], padding_value=0)\n",
    "                        }\n",
    "        return {'x': batch_x, 'y': batch_y, 'bert_x': batch_bert_x}\n",
    "\n",
    "data = DataLoader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.Wa = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = torch.nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys))).squeeze(2)\n",
    "        weights = torch.nn.functional.softmax(scores, dim=0).permute(1, 0).unsqueeze(1)\n",
    "        context = torch.bmm(weights, keys.permute(1, 0, 2))\n",
    "\n",
    "        return context.permute(1, 0, 2)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, embedding = None, input_size = len(data.x_vocab), embedding_size = embedding_size, hidden_size = hidden_size, n_layers = n_layers, dtype = torch.float):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, bidirectional=True, dropout=dropout, dtype=self.dtype)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        if embedding == \"glove\":\n",
    "            emb = torchtext.vocab.GloVe(name='6B', dim=embedding_size)\n",
    "            new_vectors = []\n",
    "            mean, std = torch.mean(emb.vectors), torch.std(emb.vectors)\n",
    "            for ind in range(input_size):\n",
    "                word = data.x_vocab[ind]\n",
    "                if word in emb.stoi:\n",
    "                    new_vectors.append(emb.vectors[emb.stoi[word]])\n",
    "                else:\n",
    "                    new_vectors.append(mean + std*torch.randn((embedding_size,)))\n",
    "            self.embedding.weight.data = torch.FloatTensor(torch.stack(new_vectors))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class BertEncoder(torch.nn.Module):\n",
    "    def __init__(self, freeze, hidden_size=hidden_size, n_layers=n_layers, device=device, dtype=torch.float):\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear = torch.nn.Linear(768, 2*n_layers*hidden_size)\n",
    "        self.linear_2 = torch.nn.Linear(768, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, mask = x['x'].to(self.device), x['a_mask'].to(self.device)\n",
    "        input = x.permute(1, 0)\n",
    "        mask = mask.permute(1, 0)\n",
    "        token_outputs, pooled_outputs = self.bert(input, attention_mask=mask, return_dict=False)\n",
    "        token_outputs = self.linear_2(token_outputs)\n",
    "        linear_output = self.linear(self.dropout(pooled_outputs))\n",
    "\n",
    "        outputs = token_outputs.permute(1, 0, 2)\n",
    "        hidden = linear_output.reshape(2*self.n_layers, -1, self.hidden_size)\n",
    "        cell = linear_output.reshape(2*self.n_layers, -1, self.hidden_size)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, attention = False, output_size = len(data.y_vocab), embedding_size = embedding_size, hidden_size = 2*hidden_size, n_layers = n_layers, dtype = torch.float):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.attention = attention\n",
    "        self.embedding = torch.nn.Embedding(output_size, embedding_size)\n",
    "        if self.attention:\n",
    "            self.lstm = torch.nn.LSTM(input_size=embedding_size+hidden_size, hidden_size=hidden_size, num_layers=n_layers, dropout=dropout, dtype=self.dtype)\n",
    "        else:\n",
    "            self.lstm = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, dropout=dropout, dtype=self.dtype)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        if attention:\n",
    "            self.att = Attention(hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, enc_out = None):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        if self.attention:\n",
    "            query = hidden[-1].unsqueeze(0)\n",
    "            keys = enc_out\n",
    "            context = self.att(query, keys)\n",
    "            dec_inp = torch.concat((embedded, context), dim=-1)\n",
    "        else:\n",
    "            dec_inp = embedded\n",
    "        out, (hidden, cell) = self.lstm(dec_inp, (hidden, cell))\n",
    "        pred = self.fc(out.squeeze(0))\n",
    "        return pred, hidden, cell\n",
    "\n",
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, tf_ratio = 0.6, device = device, dtype = torch.float):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target, tf_ratio = None):\n",
    "        # Preparing output\n",
    "        outputs = torch.zeros(target.shape[0], target.shape[1], len(data.y_vocab), device=self.device)\n",
    "\n",
    "        # Encoding\n",
    "        enc_output, hidden, cell = self.encoder(input)\n",
    "\n",
    "        # Preparing Input for decoder\n",
    "        n_layer = hidden.shape[0]//2\n",
    "        hidden = torch.concat((hidden[:n_layers], hidden[n_layer:]), dim=-1)\n",
    "        cell = torch.concat((cell[:n_layers], cell[n_layer:]), dim=-1)\n",
    "        dec_input = target[0, :]\n",
    "\n",
    "        # Setting Teacher Forcing ratio\n",
    "        if tf_ratio is None:\n",
    "            tf_ratio = self.tf_ratio\n",
    "\n",
    "        # Decoding\n",
    "        for t in range(1, target.shape[0]):\n",
    "            dec_output, hidden, cell = self.decoder(dec_input, hidden, cell, enc_output)\n",
    "            outputs[t] = dec_output\n",
    "\n",
    "            # Teacher Forcing\n",
    "            dec_input = target[t] if random.random() < tf_ratio else dec_output.argmax(1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Seq2Seq, dataloader, bert = False):\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_ct = 0\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=data.y_stoi[PAD])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Loading Data\n",
    "            if bert:\n",
    "                x = batch['bert_x']\n",
    "            else:\n",
    "                x = batch['x'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "\n",
    "            # Getting Output\n",
    "            y_pred = model(x, y, 0)\n",
    "\n",
    "            # Calculating Loss\n",
    "            y_pred = y_pred[1:].reshape(-1, y_pred.shape[-1])\n",
    "            y = y[1:].reshape(-1)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # Adding to total loss\n",
    "            epoch_loss += loss.item()\n",
    "            batch_ct += 1\n",
    "        \n",
    "    return epoch_loss/batch_ct\n",
    "\n",
    "def train(model: Seq2Seq, data: DataLoader, model_address: str, lr=lr, bert = False):\n",
    "    start_time = time.time()\n",
    "    print(\"learning rate:\", lr)\n",
    "\n",
    "    # Training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=data.y_stoi[PAD])\n",
    "    loss_arr =[[], []]\n",
    "\n",
    "    # Early Stopping\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "\n",
    "        # Loss\n",
    "        epoch_loss = 0\n",
    "        batch_ct = 0\n",
    "\n",
    "        for batch in data.train:\n",
    "            # Loading Data\n",
    "            if bert:\n",
    "                x = batch['bert_x']\n",
    "            else:\n",
    "                x = batch['x'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "\n",
    "            # Getting Output\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x, y)\n",
    "\n",
    "            # Calculating Loss\n",
    "            y_pred = y_pred[1:].reshape(-1, y_pred.shape[-1])\n",
    "            y = y[1:].reshape(-1)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # Back Propagation with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Adding to total loss\n",
    "            epoch_loss += loss.item()\n",
    "            batch_ct += 1\n",
    "\n",
    "        print(f\"Epoch: {epoch} Loss: {epoch_loss/batch_ct}\\tTime: {time.time()-start_time}\")\n",
    "\n",
    "        dev_loss = evaluate(model, data.dev, bert=bert)\n",
    "        if dev_loss < best_loss:\n",
    "            best_loss = dev_loss\n",
    "            torch.save(model.state_dict(), os.path.join(model_address, 'param.pt'))\n",
    "\n",
    "        # Updating loss array\n",
    "        loss_arr[0].append(epoch_loss/batch_ct)\n",
    "        loss_arr[1].append(dev_loss)\n",
    "\n",
    "        print(f\"Validation Loss: {dev_loss}\\n\")\n",
    "    \n",
    "    return best_loss, loss_arr\n",
    "\n",
    "def beam_search(model: Seq2Seq, data: DataLoader, dev_add, test_add, beam_size = 10, bert = False):\n",
    "    # Defining Beam Search Per Batch\n",
    "    def beam_search_per_batch(batch):\n",
    "        # Loading Data\n",
    "        if bert:\n",
    "            x = batch['bert_x']\n",
    "        else:\n",
    "            x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "\n",
    "        # Getting Output\n",
    "        y_pred = model(x, y, 0)\n",
    "        y_pred = prob(y_pred[1:])\n",
    "        y_top_val, y_top_ind = torch.topk(y_pred, beam_size)\n",
    "        \n",
    "        # # Greedy Approach\n",
    "        if beam_size == 1:\n",
    "            y_pred = y_pred.argmax(-1)\n",
    "            sentence_arr = []\n",
    "            for ind in range(y_pred.shape[1]):\n",
    "                sequence = []\n",
    "                for token in y_pred[:, ind]:\n",
    "                    word = data.y_vocab[token]\n",
    "                    sequence.append(word)\n",
    "                    if word == EOS:\n",
    "                        break\n",
    "                sequence = [word for word in sequence if word not in [SOS, EOS, UNK, PAD]]\n",
    "                sentence_arr.append(\"|\".join(sequence))\n",
    "            return sentence_arr\n",
    "\n",
    "        # Converting to sentence\n",
    "        sentence_arr = []\n",
    "        for ind in range(y_pred.shape[1]):\n",
    "            sequences = [(0, [])]\n",
    "            for t in range(y_pred.shape[0]):\n",
    "                new_sequences = []\n",
    "                for loss, seq in sequences:\n",
    "                    if seq and seq[-1] == EOS:\n",
    "                        new_loss = loss\n",
    "                        if len(new_sequences) >= beam_size and new_loss > new_sequences[-1][0]:\n",
    "                            continue\n",
    "                        new_seq = list(seq)\n",
    "                        elem = (new_loss, new_seq)\n",
    "                        # for ind in range(len(new_sequences)):\n",
    "                        #     if elem[0] < new_sequences[ind][0]:\n",
    "                        #         new_sequences[ind], elem = elem, new_sequences[ind]\n",
    "                        # if len(new_sequences) != beam_size:\n",
    "                        new_sequences.append(elem)\n",
    "                        new_sequences.sort()\n",
    "                        new_sequences = new_sequences[:beam_size]\n",
    "                    else:\n",
    "                        for word_ind in y_top_ind[t, ind, :]:\n",
    "                            word = data.y_vocab[word_ind]\n",
    "                            new_loss = loss - y_pred[t, ind, data.y_stoi[word]].item()\n",
    "                            if len(new_sequences) >= beam_size and new_loss > new_sequences[-1][0]:\n",
    "                                continue\n",
    "                            new_seq = seq + [word]\n",
    "                            elem = (new_loss, new_seq)\n",
    "                            # for ind in range(len(new_sequences)):\n",
    "                            #     if elem[0] < new_sequences[ind][0]:\n",
    "                            #         new_sequences[ind], elem = elem, new_sequences[ind]\n",
    "                            # if len(new_sequences) != beam_size:\n",
    "                            new_sequences.append(elem)\n",
    "                            new_sequences.sort()\n",
    "                            new_sequences = new_sequences[:beam_size]\n",
    "                sequences = new_sequences\n",
    "            best_sequence = [word for word in sequences[0][1] if word not in [SOS, EOS, UNK, PAD]]\n",
    "            sentence = \"|\".join(best_sequence)\n",
    "            sentence_arr.append(sentence)\n",
    "            # print(sentence)\n",
    "            ct[0] += 1\n",
    "        print(f\"Sentence: {ct[0]}/{tot}\\tTime: {time.time() - start_time}\")\n",
    "        return sentence_arr\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # For Calculating log softmax\n",
    "    prob = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ct = [0]\n",
    "        tot = len(data.data_dev) + len(data.data_test)\n",
    "        # Test\n",
    "        sentences_test = []\n",
    "        for batch in data.test:\n",
    "            sentences_test.extend(beam_search_per_batch(batch))\n",
    "        test_data = json.load(open(raw_test, 'rb'))\n",
    "        for i in range(len(sentences_test)):\n",
    "            test_data[i]['predicted'] = sentences_test[i]\n",
    "        json.dump(test_data, open(test_add, 'w'))\n",
    "\n",
    "        # Dev\n",
    "        sentences_dev = []\n",
    "        for batch in data.dev:\n",
    "            sentences_dev.extend(beam_search_per_batch(batch))\n",
    "        dev_data = json.load(open(raw_dev, 'rb'))\n",
    "        for i in range(len(sentences_dev)):\n",
    "            dev_data[i]['predicted'] = sentences_dev[i]\n",
    "        json.dump(dev_data, open(dev_add, 'w'))\n",
    "        \n",
    "def learn_model(model: Seq2Seq, model_add, model_name, bert = False, lr=lr):\n",
    "    def plot(arr, title):\n",
    "        fig, ax = plt.subplots()\n",
    "        arr_x = list(range(1, num_epoch+1))\n",
    "        ax.plot(arr_x, arr[0], label = 'Train')\n",
    "        ax.plot(arr_x, arr[1], label = 'Validation')\n",
    "        ax.set_xlabel(\"num_epochs\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        plt.savefig(os.path.join(model_add, 'loss_curve'))\n",
    "\n",
    "    val_loss, loss_arr = train(model, data, model_add, bert=bert, lr=lr)\n",
    "    plot(loss_arr, model_name)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_add, 'param.pt')))\n",
    "    val_loss = evaluate(model, data.dev, bert=bert)\n",
    "    print(\"Val Loss:\", val_loss)\n",
    "    test_loss = evaluate(model, data.test, bert=bert)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    dev_add, test_add = os.path.join(model_add, 'dev.json'), os.path.join(model_add, 'test.json')\n",
    "    beam_search(model, data, dev_add, test_add, bert=bert)\n",
    "    print(\"Test:\")\n",
    "    acc_test = evaluator.main(os.path.join(model_add, 'test.json'))\n",
    "    print(\"Val:\")\n",
    "    acc_dev = evaluator.main(os.path.join(model_add, 'dev.json'))\n",
    "    with open(os.path.join(model_add, 'report_pc.txt'), 'w') as report:\n",
    "        report.write(f\"Val Loss: {val_loss}\\n\")\n",
    "        report.write(f\"Exact Val Accuracy: {acc_dev[1]}\\n\")\n",
    "        report.write(f\"Execution Val Accuracy: {acc_dev[0]}\\n\")\n",
    "        report.write(f\"Test Loss: {test_loss}\\n\")\n",
    "        report.write(f\"Exact Test Accuracy: {acc_test[1]}\\n\")\n",
    "        report.write(f\"Execution Test Accuracy: {acc_test[0]}\\n\")\n",
    "        report.write('\\nLoss Arr:\\n')\n",
    "        # report.write(str(loss_arr))\n",
    "        report.write('\\nHyperParameters:\\n')\n",
    "        report.write(f'\\tNum Epochs: {num_epoch}\\n')\n",
    "        report.write(f\"\\tEmbedding Dim: {embedding_size}\\n\")\n",
    "        report.write(f\"\\tHidden Dim: {hidden_size}\\n\")\n",
    "        report.write(f\"\\tLearning Rate: {lr}\\n\")\n",
    "        report.write(f\"\\tDropout Probability: {dropout}\\n\")\n",
    "        report.write(f\"\\tNum Layers: {n_layers}\\n\")\n",
    "        report.write(f\"\\tGradient Clip: {clip}\\n\")\n",
    "\n",
    "def access_model_beam_size(model: Seq2Seq, model_add, bert = False):\n",
    "    for beam_size in {1, 10, 20}:\n",
    "        model.load_state_dict(torch.load(os.path.join(model_add, 'param.pt')))\n",
    "        val_loss = evaluate(model, data.dev, bert=bert)\n",
    "        print(\"Val Loss:\", val_loss)\n",
    "        test_loss = evaluate(model, data.test, bert=bert)\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        dev_add, test_add = os.path.join(model_add, 'dev.json'), os.path.join(model_add, 'test.json')\n",
    "        beam_search(model, data, dev_add, test_add, bert=bert, beam_size=beam_size)\n",
    "        print(\"Test:\")\n",
    "        acc_test = evaluator.main(os.path.join(model_add, 'test.json'))\n",
    "        print(\"Val:\")\n",
    "        acc_dev = evaluator.main(os.path.join(model_add, 'dev.json'))\n",
    "        with open(os.path.join(model_add, f'report_{beam_size}.txt'), 'w') as report:\n",
    "            report.write(f\"Val Loss: {val_loss}\\n\")\n",
    "            report.write(f\"Exact Val Accuracy: {acc_dev[1]}\\n\")\n",
    "            report.write(f\"Execution Val Accuracy: {acc_dev[0]}\\n\")\n",
    "            report.write(f\"Test Loss: {test_loss}\\n\")\n",
    "            report.write(f\"Exact Test Accuracy: {acc_test[1]}\\n\")\n",
    "            report.write(f\"Execution Test Accuracy: {acc_test[0]}\\n\")\n",
    "            report.write('\\nHyperParameters:\\n')\n",
    "            report.write(f'\\tNum Epochs: {num_epoch}\\n')\n",
    "            report.write(f\"\\tEmbedding Dim: {embedding_size}\\n\")\n",
    "            report.write(f\"\\tHidden Dim: {hidden_size}\\n\")\n",
    "            report.write(f\"\\tLearning Rate: {lr}\\n\")\n",
    "            report.write(f\"\\tDropout Probability: {dropout}\\n\")\n",
    "            report.write(f\"\\tNum Layers: {n_layers}\\n\")\n",
    "            report.write(f\"\\tGradient Clip: {clip}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove Model\n",
    "model_glove = Seq2Seq(Encoder(embedding=\"glove\"), Decoder(), device=device).to(device)\n",
    "learn_model(model_glove, model_glove_add, 'Seq2Seq model with GloVe embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove Model with Attention\n",
    "model_glove_attention2 = Seq2Seq(Encoder(embedding=\"glove\"), Decoder(attention=True), device=device).to(device)\n",
    "learn_model(model_glove_attention2, model_glove_attention_add2, 'Seq2Seq+Attention model with GloVe embeddings, tf_ratio = 0.6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of Teacher Forcing\n",
    "model_glove_attention1 = Seq2Seq(Encoder(embedding=\"glove\"), Decoder(attention=True), tf_ratio=0.3, device=device).to(device)\n",
    "learn_model(model_glove_attention1, model_glove_attention_add1, 'Seq2Seq+Attention model with GloVe embeddings, tf_ratio = 0.3')\n",
    "\n",
    "model_glove_attention3 = Seq2Seq(Encoder(embedding=\"glove\"), Decoder(attention=True), tf_ratio=0.9, device=device).to(device)\n",
    "learn_model(model_glove_attention3, model_glove_attention_add3, 'Seq2Seq+Attention model with GloVe embeddings, tf_ratio = 0.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "\n",
    "# Frozen BERT Model with Attention\n",
    "model_bert_frozen = Seq2Seq(BertEncoder(freeze=True), Decoder(), device=device).to(device)\n",
    "learn_model(model_bert_frozen, model_bert_frozen_add, \"A Seq2Seq+Attention model using a pre-trained frozen BERT-base-cased Encoder\", bert=True, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive BERT Model with Attention\n",
    "model_bert_adaptive = Seq2Seq(BertEncoder(freeze=False), Decoder(), device=device).to(device)\n",
    "learn_model(model_bert_adaptive, model_bert_adaptive_add, \"A Seq2Seq+Attention model using a pre-trained adaptive BERT-base-cased Encoder\", bert=True, lr=lr)\n",
    "access_model_beam_size(model_bert_adaptive, model_bert_adaptive_add, bert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_model_beam_size(model_glove_attention2, model_glove_attention_add2, bert=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
