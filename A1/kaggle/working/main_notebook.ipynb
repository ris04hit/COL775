{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T13:14:24.697408Z","iopub.status.busy":"2024-03-12T13:14:24.696883Z","iopub.status.idle":"2024-03-12T13:14:27.983413Z","shell.execute_reply":"2024-03-12T13:14:27.982487Z","shell.execute_reply.started":"2024-03-12T13:14:24.697360Z"},"trusted":true},"outputs":[],"source":["# imports\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import torch\n","import torchvision\n","import torchmetrics\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utils\n","\n","# Creating directory\n","def create_dir(addr):\n","    if not os.path.exists(addr):\n","        os.mkdir(addr)\n","\n","# Delete folder and its content\n","def remove_folder_contents(folder):\n","    for the_file in os.listdir(folder):\n","        file_path = os.path.join(folder, the_file)\n","        try:\n","            if os.path.isfile(file_path):\n","                os.unlink(file_path)\n","            elif os.path.isdir(file_path):\n","                remove_folder_contents(file_path)\n","                os.rmdir(file_path)\n","        except Exception as e:\n","            print(e)\n","\n","# Addresses\n","\n","# Data Address\n","data_address = \"data\"\n","\n","# Raw data\n","raw_address = \"../input/indian-birds/Birds_25\"\n","raw_train = \"../input/indian-birds/Birds_25/train\"\n","raw_test = \"../input/indian-birds/Birds_25/test\"\n","raw_val = \"../input/indian-birds/Birds_25/val\"\n","\n","# Processed data\n","processed_address = \"data/processed\"\n","cat_address = \"data/processed/cat.pkl\"\n","stat_address = \"data/processed/stats.npz\"\n","\n","# Temp Address\n","temp_address = \"temp\"\n","\n","# Results\n","result_address = \"results/\"\n","base_model_result = \"results/base_model\"\n","\n","# Init Structure\n","create_dir(temp_address)\n","create_dir(data_address)\n","create_dir(processed_address)\n","create_dir(raw_address)\n","create_dir(raw_train)\n","create_dir(raw_test)\n","create_dir(raw_val)\n","create_dir(result_address)\n","# remove_folder_contents(result_address)\n","\n","# Constants\n","random_seed = 68\n","n = 2\n","r = 25\n","batch_size = 32\n","\n","# Setting Random Seed\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Statistical Calculation\n","\n","# Loading statistical data\n","def load_stat(stat_addr):\n","    x_stats = np.load(stat_addr)\n","    x_mean = x_stats['mean']\n","    x_std = x_stats['std']\n","    return x_mean, x_std\n","\n","# Saving statistical data\n","def save_stat(lin_sum, quad_sum, stat_addr, total_ct):\n","    x_mean = (lin_sum/total_ct).astype(np.float64)\n","    x_std = np.sqrt(quad_sum/total_ct - x_mean**2).astype(np.float64)\n","    np.savez_compressed(stat_addr, mean = x_mean, std = x_std)\n","    print(\"Saved statistical processed data\")\n","\n","# Partition data into smaller fragments and returns sum and quad sum\n","def moment_data(category, shape, address, stat_addr, mode):\n","    # Reading Data and Preprocessing it\n","    lin_sum, quad_sum = np.zeros(shape, dtype=np.float64), np.zeros(shape, dtype=np.float64)\n","    ct_list = []\n","    for cat in category:\n","        cat_path = os.path.join(address, cat)\n","        ct_list.append(len(os.listdir(cat_path)))\n","        \n","        # Checking if already present\n","        if os.path.exists(stat_addr) or mode==1:\n","            continue\n","        \n","        # Reading each image of particular category\n","        count = 0\n","        for img_name in sorted(os.listdir(cat_path)):\n","            img_path = os.path.join(cat_path, img_name)\n","            img = np.array(cv2.imread(img_path))\n","            lin_sum += img.astype(np.float64)\n","            quad_sum += np.square(img.astype(np.float64))\n","            count += 1\n","        \n","        # log\n","        print(f\"Read {cat}\\tindex: {category.index(cat)}\\tnum: {count}\")\n","\n","    if mode == 0:\n","        print(\"Read Train Data\")\n","    else:\n","        print(\"Read Test Data\")\n","\n","    return lin_sum, quad_sum, ct_list\n","\n","# Reading data fragments and normalizing them\n","def normalize(addr, category, processed_x_addr, processed_y_addr, norm, overwrite):\n","    ct = 0\n","    for cat in category:\n","        cat_addr = os.path.join(addr, cat)\n","        for img_name in sorted(os.listdir(cat_addr)):\n","            # Save Address\n","            x_addr = os.path.join(processed_x_addr, f'{ct}.pt')\n","            y_addr = os.path.join(processed_y_addr, f'{ct}.pt')\n","            if not overwrite and os.path.exists(x_addr) and os.path.exists(y_addr):\n","                ct += 1\n","                continue\n","            \n","            # Reading image\n","            img_path = os.path.join(cat_addr, img_name)\n","            img = np.array(cv2.imread(img_path), dtype=np.float64)\n","            \n","            # Normalizing image\n","            norm_img = norm(img)\n","            x = torch.tensor(norm_img)\n","            \n","            # Getting y\n","            y = np.zeros((len(category),), dtype=np.float64)\n","            y[category.index(cat)] = 1\n","            y = torch.Tensor(y)\n","            \n","            # Saving image\n","            torch.save(x, x_addr)\n","            torch.save(y, y_addr)\n","            \n","            # Incrementing counter\n","            ct += 1\n","        \n","        print(f\"Preprocessed\\tindex: {category.index(cat)}\\t{cat}\")\n","\n","# Preprocessed data\n","def preprocess(address, mode, overwrite = False, cat_addr = None, stat_addr = None, shape = (256, 256, 3)):\n","    '''\n","    mode = 0: Training Mode\n","    mode = 1: Testing Mode\n","    '''\n","\n","    if mode not in [0, 1]:\n","        raise Exception(\"Not a Valid Mode\")\n","\n","    # Identifying Categories\n","    if mode == 0:\n","        category = sorted(os.listdir(address))\n","        \n","        # Saving categories\n","        if overwrite or not os.path.exists(cat_addr):\n","            with open(cat_addr, 'wb') as cat_file:\n","                pickle.dump(category, cat_file)\n","        print(\"Assigned Categories\")\n","        \n","    elif mode == 1:\n","        # Reading Category array\n","        with open(cat_addr, 'rb') as cat_file:\n","            category = pickle.load(cat_file)\n","        print(\"Read Categories\")\n","    \n","    # count of images and their moments\n","    lin_sum, quad_sum, ct_list = moment_data(category=category, shape=shape, address=address, stat_addr=stat_addr, mode=mode)\n","        \n","    # Normalizing Data\n","    if mode == 0 and (overwrite or not os.path.exists(stat_addr)):\n","        save_stat(lin_sum=lin_sum, quad_sum=quad_sum, stat_addr=stat_addr, total_ct=sum(ct_list))\n","    x_mean, x_std = load_stat(stat_addr)\n","        \n","    # # Function for normalization\n","    # norm = lambda img: np.divide((img - x_mean), x_std, out = np.zeros_like(x_mean), where = x_std!=0)\n","\n","    # # Creating train directory\n","    # create_dir(processed_x_addr)\n","    # create_dir(processed_y_addr)\n","\n","    # # Reading Saved Files and Normalizing them\n","    # normalize(addr=address, category=category, processed_x_addr=processed_x_addr, processed_y_addr=processed_y_addr, norm=norm, overwrite=overwrite)\n","\n","    return category\n","\n","# Function Call\n","category = preprocess(address=raw_train, mode=0, overwrite=False, cat_addr=cat_address, stat_addr=stat_address)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cuda\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Working with {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data Loader\n","\n","# Loading statistical data\n","x_mean, x_std = load_stat(stat_address)\n","x_mean = np.mean(np.mean(x_mean, axis=0), axis=0)\n","x_std = np.mean(np.mean(x_std, axis=0), axis=0)\n","\n","# Transformation for preprocessing\n","transform_augment = torchvision.transforms.Compose([\n","                    torchvision.transforms.RandomHorizontalFlip(),\n","                    torchvision.transforms.RandomVerticalFlip(),\n","                    torchvision.transforms.RandomAutocontrast(),\n","                    torchvision.transforms.ToTensor(),\n","                    torchvision.transforms.Normalize(mean = x_mean, std = x_std)\n","                    ])\n","\n","transform_normal = torchvision.transforms.Compose([\n","                   torchvision.transforms.ToTensor(),\n","                   torchvision.transforms.Normalize(mean = x_mean, std = x_std)\n","                   ])\n","\n","class DataSet(torch.utils.data.Dataset):\n","    def __init__(self, address, augment = True):\n","        self.address = address\n","        self.cat_list = []\n","        for cat in category:\n","            cat_path = os.path.join(self.address, cat)\n","            self.cat_list.append(sorted(os.listdir(cat_path)))\n","        self.transform = transform_augment if augment else transform_normal\n","\n","    def __len__(self):\n","        return sum([len(elem) for elem in self.cat_list])\n","    \n","    def __getitem__(self, idx):\n","        ind = idx\n","        for cat_ind in range(len(self.cat_list)):\n","            if ind < len(self.cat_list[cat_ind]):\n","                cat_path = os.path.join(self.address, category[cat_ind])\n","                img_path = os.path.join(cat_path, self.cat_list[cat_ind][ind])\n","                y_vec = np.zeros((len(category),))\n","                y_vec[cat_ind] = 1\n","                y = torch.tensor(y_vec, dtype=torch.float64)\n","                break\n","            else:\n","                ind -= len(self.cat_list[cat_ind])\n","        img = torchvision.datasets.folder.default_loader(img_path)\n","        x = self.transform(img).to(torch.float64)\n","        return x, y\n","\n","# Training Dataset and Data Loader\n","dataset_train = DataSet(raw_train)\n","data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = 4)\n","\n","# Validation Dataset and Data Loader\n","dataset_val = DataSet(raw_val, augment=False)\n","data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers = 4)\n","\n","# Test Dataset and Data Loader\n","dataset_test = DataSet(raw_test, augment=False)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model\n","\n","class ResBlock(torch.nn.Module):\n","    def __init__(self, in_channel, out_channel, norm_func, kernel_size=3, stride=1):\n","        super(ResBlock, self).__init__()\n","        \n","        self.conv1 = torch.nn.Conv2d(in_channel, in_channel, kernel_size, stride=1, padding=1, dtype=torch.float64)\n","        self.norm1 = norm_func(in_channel, dtype=torch.float64)\n","        self.activation1 = torch.nn.ReLU()\n","        \n","        self.conv2 = torch.nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding=1, dtype=torch.float64)\n","        self.norm2 = norm_func(out_channel, dtype=torch.float64)\n","        self.activation2 = torch.nn.ReLU()\n","        \n","        self.project = True if (in_channel != out_channel) or (stride != 1) else False\n","        if self.project:\n","            self.conv_project = torch.nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding=1, dtype=torch.float64)\n","\n","    def forward(self, x):\n","        res = x\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x = self.activation1(x)\n","        \n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x += self.conv_project(res) if self.project else res\n","        x = self.activation2(x)\n","        \n","        return x\n","\n","class Resnet(torch.nn.Module):\n","    def __init__(self, n, r, norm_func = torch.nn.BatchNorm2d):\n","        super(Resnet, self).__init__()\n","\n","        self.norm = norm_func\n","        \n","        #Input\n","        self.input_layer = []\n","        self.input_layer.append(torch.nn.Conv2d(3, 16, 3, 1, padding=1, dtype=torch.float64))\n","        self.input_layer.append(self.norm(16, dtype=torch.float64))\n","        self.input_layer.append(torch.nn.ReLU())\n","        self.input_layer = torch.nn.Sequential(*self.input_layer)\n","        \n","        # Layer1\n","        self.hidden_layer1 = []\n","        for i in range(n):\n","            self.hidden_layer1.append(ResBlock(16, 16, self.norm))\n","        self.hidden_layer1 = torch.nn.Sequential(*self.hidden_layer1)\n","        \n","        # Layer2\n","        self.hidden_layer2 = []\n","        self.hidden_layer2.append(ResBlock(16, 32, self.norm, stride = 2))\n","        for i in range(n-1):\n","            self.hidden_layer2.append(ResBlock(32, 32, self.norm))\n","        self.hidden_layer2 = torch.nn.Sequential(*self.hidden_layer2)\n","        \n","        # Layer3\n","        self.hidden_layer3 = []\n","        self.hidden_layer3.append(ResBlock(32, 64, self.norm, stride = 2))\n","        for i in range(n-1):\n","            self.hidden_layer3.append(ResBlock(64, 64, self.norm))\n","        self.hidden_layer3 = torch.nn.Sequential(*self.hidden_layer3)\n","            \n","        # Pool Layer\n","        self.pool = torch.nn.AdaptiveAvgPool2d(1)\n","        self.flatten = torch.nn.Flatten()\n","        \n","        # Output Layer\n","        self.output_layer = []\n","        self.output_layer.append(torch.nn.Linear(64, r, dtype=torch.float64))\n","        self.output_layer.append(torch.nn.Softmax(1))\n","        self.output_layer = torch.nn.Sequential(*self.output_layer)\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        x = self.hidden_layer1(x)\n","        x = self.hidden_layer2(x)\n","        x = self.hidden_layer3(x)\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","        x = self.output_layer(x)\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-12T13:14:28.038373Z","iopub.status.busy":"2024-03-12T13:14:28.038037Z","iopub.status.idle":"2024-03-12T13:14:28.072909Z","shell.execute_reply":"2024-03-12T13:14:28.071853Z","shell.execute_reply.started":"2024-03-12T13:14:28.038344Z"},"trusted":true},"outputs":[],"source":["# Training Model\n","\n","def train(model, data_loader, save_addr, num_epoch = 50, learning_rate = 1e-2, overwrite = False):\n","    # Creating save folder\n","    create_dir(save_addr)\n","    model_addr = os.path.join(save_addr, 'model')\n","    create_dir(model_addr)\n","    loss_addr = os.path.join(save_addr, 'loss')\n","    create_dir(loss_addr)\n","\n","    # Parameters for training\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","    start_time = time.time()\n","\n","    # Metrics\n","    metric_f1_micro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'micro').to(device)\n","    metric_f1_macro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'macro').to(device)\n","    metric_accuracy = torchmetrics.classification.Accuracy(task = 'multiclass', num_classes = r).to(device)\n","    \n","    for epoch in range(num_epoch):\n","        batch_ct = 0\n","        epoch_loss = 0\n","        label_arr = torch.tensor([], device=device)\n","        label_pred_arr = torch.tensor([], device=device)\n","\n","        # Loading previous model\n","        epoch_addr = os.path.join(model_addr, f'{0 if epoch < 10 else \"\"}{epoch}.pt')\n","        epoch_loss_addr = os.path.join(loss_addr, f'{0 if epoch < 10 else \"\"}{epoch}.pt')\n","\n","        if not overwrite and os.path.exists(epoch_addr) and os.path.exists(epoch_loss_addr):\n","            loss_arr = torch.load(epoch_loss_addr)\n","            epoch_loss = loss_arr[0].item()\n","            accuracy = loss_arr[1].item()\n","            f1_micro = loss_arr[2].item()\n","            f1_macro = loss_arr[3].item()\n","\n","            print(f\"Epoch: {epoch} Loaded\\t\\tLoss: {round(epoch_loss, 6)}\\tAccuracy: {round(accuracy, 6)}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\")\n","        else:\n","            # Training next epoch\n","            for x, y in data_loader:\n","                # To Device\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                # Predictions\n","                y_pred = model(x)\n","\n","                # Maintaining label array\n","                label, label_pred = torch.argmax(y, dim=1), torch.argmax(y_pred, dim=1)\n","                label_arr = torch.cat((label_arr, label))\n","                label_pred_arr = torch.cat((label_pred_arr, label_pred))\n","\n","                # Calculating Loss\n","                loss = loss_fn(y_pred, y)\n","                epoch_loss += loss.item()/label.numel()\n","\n","                # Back Propogation\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Log\n","                batch_ct += 1\n","                if batch_ct%100 == 0:\n","                    print(f\"\\tBatch: {batch_ct}\\tTotal Loss: {round(epoch_loss/batch_ct, 6)}\\tTime: {time.time()-start_time}\")\n","\n","            # Computing Metrics\n","            f1_micro = metric_f1_micro(label_pred_arr, label_arr).item()\n","            f1_macro = metric_f1_macro(label_pred_arr, label_arr).item()\n","            accuracy = metric_accuracy(label_pred_arr, label_arr).item()\n","            loss_arr = torch.tensor([epoch_loss/batch_ct, accuracy, f1_micro, f1_macro])\n","\n","            # Saving model after each epoch\n","            torch.save(model.state_dict(), epoch_addr)\n","            torch.save(loss_arr, epoch_loss_addr)\n","\n","            print(f\"Epoch: {epoch}\\tLoss: {round(epoch_loss/batch_ct, 6)}\\tAccuracy: {round(accuracy, 6)}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\\tTime: {time.time() - start_time}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Validate Model\n","\n","def validate(model, data_loader, save_addr, load_addr, overwrite = False):\n","    # Creating save folder\n","    create_dir(save_addr)\n","    model_addr_load = os.path.join(load_addr, 'model')\n","\n","    # Parameters for training\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    start_time = time.time()\n","\n","    # Metrics\n","    metric_f1_micro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'micro').to(device)\n","    metric_f1_macro = torchmetrics.classification.MulticlassF1Score(num_classes = r, average = 'macro').to(device)\n","    metric_accuracy = torchmetrics.classification.Accuracy(task = 'multiclass', num_classes = r).to(device)\n","\n","    epoch = 0\n","    for param_addr in sorted(os.listdir(model_addr_load)):\n","        # Save Address\n","        epoch_loss_addr = os.path.join(save_addr, param_addr)\n","\n","        # Checking if already present\n","        if not overwrite and os.path.exists(epoch_loss_addr):\n","            loss_arr = torch.load(epoch_loss_addr)\n","            epoch_loss = loss_arr[0].item()\n","            accuracy = loss_arr[1].item()\n","            f1_micro = loss_arr[2].item()\n","            f1_macro = loss_arr[3].item()\n","\n","            print(f\"Epoch: {epoch} Loaded\\t\\tLoss: {round(epoch_loss, 6)}\\tAccuracy: {round(accuracy, 6)}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\")\n","            epoch += 1\n","            continue\n","        \n","        # Initializing Variable\n","        batch_ct = 0\n","        epoch_loss = 0\n","        label_arr = torch.tensor([], device=device)\n","        label_pred_arr = torch.tensor([], device=device)\n","\n","        # Loading Model\n","        model.load_state_dict(torch.load(os.path.join(model_addr_load, param_addr)))\n","\n","        # Evaluate Model and freezing it\n","        model.eval()\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","        for x, y in data_loader:\n","            # To Device\n","            x = x.to(device)\n","            y = y.to(device)\n","            \n","            # Predictions\n","            y_pred = model(x)\n","\n","            # Maintaining label array\n","            label, label_pred = torch.argmax(y, dim=1), torch.argmax(y_pred, dim=1)\n","            label_arr = torch.cat((label_arr, label))\n","            label_pred_arr = torch.cat((label_pred_arr, label_pred))\n","\n","            # Calculating Loss\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()/label.numel()\n","\n","            # Log\n","            batch_ct += 1\n","            if batch_ct%100 == 0:\n","                print(f\"\\tBatch: {batch_ct}\\tTotal Loss: {round(epoch_loss/batch_ct, 6)}\\tTime: {time.time()-start_time}\")\n","\n","        # Computing Metrics\n","        f1_micro = metric_f1_micro(label_pred_arr, label_arr).item()\n","        f1_macro = metric_f1_macro(label_pred_arr, label_arr).item()\n","        accuracy = metric_accuracy(label_pred_arr, label_arr).item()\n","        loss_arr = torch.tensor([epoch_loss/batch_ct, accuracy, f1_micro, f1_macro])\n","\n","        # Saving model after each epoch\n","        torch.save(loss_arr, epoch_loss_addr)\n","\n","        # Log\n","        print(f\"Epoch: {epoch}\\tLoss: {round(epoch_loss/batch_ct, 6)}\\tAccuracy: {round(accuracy, 6)}\\tf1_micro: {f1_micro}\\tf1_macro: {f1_macro}\\tTime: {time.time() - start_time}\")\n","        epoch += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot\n","def save(arr_x, arr_train, arr_val, address, title, y_label):\n","    fig, ax = plt.subplots()\n","    ax.plot(arr_x, arr_train, label = 'Train')\n","    ax.plot(arr_x, arr_val, label = 'Validation')\n","    ax.set_xlabel(\"num_epochs\")\n","    ax.set_ylabel(y_label)\n","    ax.set_title(title)\n","    ax.legend()\n","    plt.savefig(address)\n","\n","def plot(train_address, val_address, save_address):\n","    # Loading metrics\n","    train_metrics = []\n","    val_metrics = []\n","    for epoch_name in sorted(os.listdir(train_address)):\n","        train_metrics.append(np.array(torch.load(os.path.join(train_address, epoch_name))))\n","        val_metrics.append(np.array(torch.load(os.path.join(val_address, epoch_name))))\n","    train_metrics = np.stack(train_metrics)\n","    val_metrics = np.stack(val_metrics)\n","\n","    arr_x = np.arange(1, train_metrics.shape[0]+1)\n","    save(arr_x, train_metrics[:, 0], val_metrics[:, 0], os.path.join(save_address, 'loss'), \"Cross Entropy Loss\", \"Loss\")\n","    save(arr_x, 100*train_metrics[:, 1], 100*val_metrics[:, 1], os.path.join(save_address, 'acc'), \"Accuracy\", \"Accuracy\")\n","    save(arr_x, train_metrics[:, 2], val_metrics[:, 2], os.path.join(save_address, 'micro'), \"F1 Micro Score\", \"f1_micro\")\n","    save(arr_x, train_metrics[:, 3], val_metrics[:, 3], os.path.join(save_address, 'macro'), \"F1 Macro Score\", \"f1_macro\")\n","\n","# Report\n","def report(address):\n","    metric_arr = []\n","    for folder_name in ['loss', 'val', 'test']:\n","        folder_address = os.path.join(address, folder_name)\n","        metric_addr = os.path.join(folder_address, sorted(os.listdir(folder_address))[-1])\n","        metric_arr.append(np.array(torch.load(metric_addr)))\n","\n","    with open(os.path.join(address, 'result.txt'), 'w') as file:\n","        file.write(\"Training:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[0][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[0][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[0][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[0][3]}\\n\")\n","        file.write(\"\\nValidation:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[1][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[1][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[1][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[1][3]}\\n\")\n","        file.write(\"\\nTesting:\\n\")\n","        file.write(f\"\\tLoss:\\t\\t{metric_arr[2][0]}\\n\")\n","        file.write(f\"\\tAccuracy:\\t{metric_arr[2][1]}\\n\")\n","        file.write(f\"\\tF1_Micro:\\t{metric_arr[2][2]}\\n\")\n","        file.write(f\"\\tF1_Macro:\\t{metric_arr[2][3]}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Base Model\n","\n","resnet_base_model = Resnet(n, r).to(device)\n","train(resnet_base_model, data_loader_train, base_model_result)\n","validate(resnet_base_model, data_loader_val, os.path.join(base_model_result, 'val'), base_model_result)\n","validate(resnet_base_model, data_loader_test, os.path.join(base_model_result, 'test'), base_model_result)\n","plot (os.path.join(base_model_result, 'loss'), os.path.join(base_model_result, 'val'), base_model_result)\n","report(base_model_result)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4584966,"sourceId":7824688,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":4}
